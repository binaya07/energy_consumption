{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqURBE7w2qoy"
   },
   "source": [
    "# Image classification with Vision Transformer\n",
    "\n",
    "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
    "**Date created:** 2021/01/18<br>\n",
    "**Last modified:** 2021/01/18<br>\n",
    "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cl8Ydp_2qo0"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
    "model by Alexey Dosovitskiy et al. for image classification,\n",
    "and demonstrates it on the CIFAR-100 dataset.\n",
    "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
    "image patches, without using convolution layers.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
    "which can be installed using the following command:\n",
    "\n",
    "```python\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLevqRD42qo1"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLDXayUc2qo1",
    "outputId": "fc648e1a-295a-4e08-8951-55e9ea145476",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Akf-Y3xh2-cZ",
    "outputId": "d8bafd3d-532b-45ab-b367-b5c622cea27d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binaya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMbJTOoK2qo2"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qx0EsNPR2qo3",
    "outputId": "6dca6a31-36e5-49fa-97e8-6e625aefb881",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYNbbLOz2qo3"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hM3ivTOy2qo4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od24nz-y2qo5"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FPiii8mH2qo5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYHHJ8B92qo5"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hTpaOsl-2qo6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIAU6hfS2qo6"
   },
   "source": [
    "## Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jyBY0aq-2qo7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV0NqgMw2qo8"
   },
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4vYWKJbY2qo8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 144\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpV4POnx2qo9"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_VX660D-2qo9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDBCtyjy2qo9"
   },
   "source": [
    "## Build the ViT model\n",
    "\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZBoZ95za2qo9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier():\n",
    "    print(\"reached here\")\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    print(inputs.shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    print(augmented.shape)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached here\n",
      "(None, 32, 32, 3)\n",
      "(None, 72, 72, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2a54fa1b550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vit_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny4eLJ6G2qo-"
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OT4MAmkQ2qo-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached here\n",
      "(None, 32, 32, 3)\n",
      "(None, 72, 72, 3)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " patches_2 (Patches)            (None, None, 108)    0           ['data_augmentation[1][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 144, 64)     16192       ['patches_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 144, 64)     128         ['patch_encoder_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 144, 64)     66368       ['layer_normalization_17[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 144, 64)      0           ['multi_head_attention_8[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 144, 64)     128         ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 144, 128)     8320        ['layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 144, 128)     0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 144, 64)      8256        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 144, 64)      0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 144, 64)      0           ['dropout_20[0][0]',             \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 144, 64)     128         ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 144, 64)     66368       ['layer_normalization_19[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 144, 64)      0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 144, 64)     128         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 144, 128)     8320        ['layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 144, 128)     0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 144, 64)      8256        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 144, 64)      0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 144, 64)      0           ['dropout_22[0][0]',             \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 144, 64)     128         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 144, 64)     66368       ['layer_normalization_21[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 144, 64)      0           ['multi_head_attention_10[0][0]',\n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 144, 64)     128         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 144, 128)     8320        ['layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 144, 128)     0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 144, 64)      8256        ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 144, 64)      0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 144, 64)      0           ['dropout_24[0][0]',             \n",
      "                                                                  'add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 144, 64)     128         ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 144, 64)     66368       ['layer_normalization_23[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 144, 64)      0           ['multi_head_attention_11[0][0]',\n",
      "                                                                  'add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 144, 64)     128         ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 144, 128)     8320        ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 144, 128)     0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 144, 64)      8256        ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 144, 64)      0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 144, 64)      0           ['dropout_26[0][0]',             \n",
      "                                                                  'add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 144, 64)     128         ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 144, 64)     66368       ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 144, 64)      0           ['multi_head_attention_12[0][0]',\n",
      "                                                                  'add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 144, 64)     128         ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 144, 128)     8320        ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 144, 128)     0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 144, 64)      8256        ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 144, 64)      0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 144, 64)      0           ['dropout_28[0][0]',             \n",
      "                                                                  'add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 144, 64)     128         ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 144, 64)     66368       ['layer_normalization_27[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 144, 64)      0           ['multi_head_attention_13[0][0]',\n",
      "                                                                  'add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 144, 64)     128         ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 144, 128)     8320        ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 144, 128)     0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 144, 64)      8256        ['dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 144, 64)      0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 144, 64)      0           ['dropout_30[0][0]',             \n",
      "                                                                  'add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 144, 64)     128         ['add_27[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 144, 64)     66368       ['layer_normalization_29[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 144, 64)      0           ['multi_head_attention_14[0][0]',\n",
      "                                                                  'add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 144, 64)     128         ['add_28[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 144, 128)     8320        ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 144, 128)     0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 144, 64)      8256        ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 144, 64)      0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 144, 64)      0           ['dropout_32[0][0]',             \n",
      "                                                                  'add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 144, 64)     128         ['add_29[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 144, 64)     66368       ['layer_normalization_31[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 144, 64)      0           ['multi_head_attention_15[0][0]',\n",
      "                                                                  'add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 144, 64)     128         ['add_30[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 144, 128)     8320        ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 144, 128)     0           ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 144, 64)      8256        ['dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 144, 64)      0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 144, 64)      0           ['dropout_34[0][0]',             \n",
      "                                                                  'add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 144, 64)     128         ['add_31[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 9216)         0           ['layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 9216)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 2048)         18876416    ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 2048)         0           ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 1024)         2098176     ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 1024)         0           ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 100)          102500      ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,759,019\n",
      "Trainable params: 21,759,012\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "  1/176 [..............................] - ETA: 40:14 - loss: 7.6498 - accuracy: 0.0078 - top-5-accuracy: 0.0508"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m     41\u001b[0m vit_classifier \u001b[38;5;241m=\u001b[39m create_vit_classifier()\n\u001b[1;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit_classifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     16\u001b[0m checkpoint_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     18\u001b[0m     checkpoint_filepath,\n\u001b[0;32m     19\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_filepath)\n\u001b[0;32m     34\u001b[0m _, accuracy, top_5_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW9-L8W42qo-"
   },
   "source": [
    "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
    "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
    "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
    "\n",
    "Note that the state of the art results reported in the\n",
    "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
    "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
    "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
    "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
    "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
    "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
    "In practice, it's recommended to fine-tune a ViT model\n",
    "that was pre-trained using a large, high-resolution dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "image_classification_with_vision_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
